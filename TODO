# To do

## Backlog

* ~~Helper function to increment filenames~~
* ~~Ensure outputs are cached to correct location of revised repo structure~~
* ~~Select a logging option to capture console output~~
* ~~Need pretty sessioninfo output for logging~~
* ~~Start logging with human friendly delimeter~~
* ~~added handle_query() function with logging output~~
* ~~added wrap_up() function for ending sequential ex of munge scripts~~
* ~~Query api and create equivalents of site.midas, site.tame and site.tmu data~~
* ~~Unit tests for site dataframes~~
* ~~Daily reports missing site IDs, inspect this in webtri.sh, may need to query one site at a time~~
* ~~Investigate issue with 07-join_readings_sites.R, no readings for tame or tmu sites...~~
* ~~querying siteIds - Use http_status to skip dataframe processing for anything returning $category == "Success" && $reason == "No content", certain siteIDs missing from provided api daterange~~
* ~~querying dateranges - need to establish api requirements and exception handling~~
* ~~Output a text file containing all sites that returned empty content for the specified date range~~

* Will need to write individual report csvs to cache, remove from local memory per iteration, garbage collection and tidy environment, then compile csvs.
* Alternative to fall back on is executing shell scripts as part of R workflow.
* Combine the csvs in /data/ into `combo.csv`, as achieved in webtri.sh
* Update to documentation - markdown web page with screengrabs
* handle_missing function requires unit testing.
* Implement a retry for any internal server errors (HTTP status code 5**), script 07-GET_daily_reports.R stores these as retru_urls
* Implement partial data dump logic, when the cache file is partially populated due to an interrupted run, it would be more efficient to compare cached daily reports already ingested against those that are still outstanding. Pick up where we left off. 
* Use beepr::beep() for errors at common throttle points



## Performance

* performance 04/12/2020, Time difference of 1.126 mins, reading from csvs
* Down to Time difference of 58.281 secs reading from .Rdata